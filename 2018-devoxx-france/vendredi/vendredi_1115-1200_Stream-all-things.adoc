= Vendredi 11:15 / 12:00 - Stream All Things - Patterns of Modern Data Integration
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ../images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
//:toclevels: 3
// To turn off figure caption labels and numbers
:figure-caption!:

toc::[]

Par Gwen Shapira, Confluent, author of “Kafka - The Definitive Guide” and "Hadoop Application Architectures". +
https://cfp.devoxx.fr/2018/talk/NCH-3345/Stream_All_Things_-%5FPatterns_of_Modern_Data_Integration[Descriptif de la conférence sur le site du CFP de Devoxx] +
icon:tags[] Key words : Kafka, Stream Processing, Event-Driven Microservices

// ifdef::env-github[]
// https://www.youtube.com/watch?v=XXXXXX[vidéo de la présentation sur YouTube]
// endif::[]
// ifdef::env-browser[]
// video::XXXXXX[youtube, width=640, height=480]
// endif::[]

== Overview

====
80% of the time in every project is spent on data integration: Getting the data you want the way you want it. +
This problem remains challenging despite 40 years of attempts to solve it. +
We want a reliable, low latency system that can handle varied data from wide range of data management systems. +
We want a solution that is easy to manage and easy to scale. Is it too much to ask?

In this presentation, we’ll discuss the basic challenges of data integration and introduce design and architecture patterns that are used to tackle these challenges. +
We will explore how these patterns can be implemented using Apache Kafka and share pragmatic solutions that many engineering organizations used to build fast, scalable and manageable data pipelines.
====

== Notes

Comparison of advantages and drawbacks for DWH, datalake, Data Stream

image::1

Gwen doesn't like Informatica !

image::2

* DWH et datalake: for ETL engineers
* datalake and data stream: for software engineer

Now, software engineers replaced a whole bunch of other jobs:

image::3

They do things a bit differently:

* write code, rather than be the "Informatica Guy"
* buld apps, rather than helping people to explore data

In fact, they have multiple responsibilities (Swiss knife)

=== Stream of events

Main pain-points of data integration:

* all your data is event streams : continuous chain of events, the whole being *immutable*

Data integration is to collect, process and store data

Managing hundreds of different event types is insane. +
-> a solution: put all streams in *One* Kafka

Put everything in one place is a good pattern -> it avoids the need of connecting to hundreds of different stores

.Recap
image::4

La structure de log de Kafka est un gros avantage qui permet à Kafka de scaller facilement.

In Kafka, producers are totally unaware of what the consumers do with the data. +
This can be a problem (you can easily break a lot of consumers with broadcasting a bad change)

Exemple

image::5

IMPORTANT: Stream processing world : Event schemas ARE the API

image::6

image::7

A reminder of Kafka scalable model

image::8

Hipster Stream Processing : a very simple framework. +
"very simple" meaning *NOT* for all use cases

Kafka Connect

image::9

A (very) big bunch of connectors to connect Kafka to sources and sinks.

WARNING: Don't take this pattern too far! +
Too far meaning *state management*

Example of Kafka workflow - initial one

image::10

Update 1

image::11

But *latency problems* with the connection with the DB (not enough throughput)

Update 2

On ajoute un cache pour éviter les problèmess de latence côté BDD. +
Mais vient alors le pb : *quand dois-je invalider le cache ?*

Update 3

image::12

We store the event log from the DB in Kafka (we do CDC)

image::13

Dans les faits, il y a plusieurs grandes questions à se poser 

image::14

* Si cela ne tient pas en mémoire, je dois sharder
* maintenir un cache distribué n'est pas simple
* comment persister l'état ? (avec la résilience qui va avec ?)
* *comment gérer des co-partitions pour les JOIN ?*

-> Pour tout cela, il y a *KafkaStreams*



== Avis

Gwen parle plus vite que Lucky Luke ne tire...
